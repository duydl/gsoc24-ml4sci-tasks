{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Quantum Generative Adversarial Network (QGAN)\n",
    "\n",
    "You will explore how best to apply a quantum generative adversarial network (QGAN) to solve a High Energy Data analysis issue, more specifically, separating the signal events from the background events. You should use the Google Cirq and Tensorflow Quantum (TFQ) libraries for this task. \n",
    "\n",
    "A set of input samples (simulated with Delphes) is provided in NumPy NPZ format [Download Input]. In the input file, there are only 100 samples for training and 100 samples for testing so it won’t take much computing resources to accomplish this \n",
    "task. The signal events are labeled with 1 while the background events are labeled with 0. \n",
    "Be sure to show that you understand how to fine tune your machine learning model to improve the performance. The performance can be evaluated with classification accuracy or Area Under ROC Curve (AUC). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43079088  0.86834819 -0.92614721 -0.92662029 -0.56900862]\n",
      " [ 0.33924198  0.56155499  0.93097459 -0.91631726 -0.54463516]\n",
      " [-0.42888879  0.87064961 -0.92782179 -0.77533991 -0.58329176]\n",
      " [-0.43262871  0.86128919 -0.92240878 -0.88048862 -0.49963115]\n",
      " [-0.99925345 -0.99949586  0.07753685 -0.84218034 -0.5149399 ]]\n",
      "[[ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]]\n",
      "(100, 5) (100, 3)\n",
      "(100, 5) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/QIS_EXAM_200Events.npz', allow_pickle=True)\n",
    "X_train=data['training_input'].item()\n",
    "X_test=data['test_input'].item()\n",
    "\n",
    "X_train_0 = X_train['0']\n",
    "X_train_1 = X_train['1']\n",
    "X_test_0 = X_test['0']\n",
    "X_test_1 = X_test['1']\n",
    "\n",
    "X_train = np.concatenate((X_train_0,X_train_1),axis=0)\n",
    "X_test = np.concatenate((X_test_0,X_test_1),axis=0)\n",
    "\n",
    "y_train = np.zeros((len(X_train),1))\n",
    "y_train[len(X_train_0):] = 1\n",
    "y_test = np.zeros((len(X_test),))\n",
    "y_test[len(X_test_0):] = 1\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)*2-1\n",
    "y_test = tf.keras.utils.to_categorical(y_test)*2-1\n",
    "\n",
    "y_train_hinge = np.concatenate((y_train, np.ones((len(y_train), 1))), axis=1)\n",
    "y_test_hinge = np.concatenate((y_test, np.ones((len(y_test), 1))), axis=1)\n",
    "\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train_hinge[:5])\n",
    "\n",
    "print(X_train.shape, y_train_hinge.shape)\n",
    "print(X_test.shape, y_test_hinge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0): ───X^(RY1:0)───Y^(RX1:0)───Z^(RZ1:0)───@───────────────────────\n",
      "                                               │\n",
      "(0, 1): ───X^(RY1:1)───Y^(RX1:1)───Z^(RZ1:1)───X───@───────────────────\n",
      "                                                   │\n",
      "(0, 2): ───X^(RY1:2)───Y^(RX1:2)───Z^(RZ1:2)───────X───@───────────────\n",
      "                                                       │\n",
      "(0, 3): ───X^(RY1:3)───Y^(RX1:3)───Z^(RZ1:3)───────────X───@───────────\n",
      "                                                           │\n",
      "(0, 4): ───X^(RY1:4)───Y^(RX1:4)───Z^(RZ1:4)───────────────X───@───────\n",
      "                                                               │\n",
      "0: ────────────────────────────────────────────────────────────X───H───\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " pqc_18 (PQC)                (None, 1)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 98ms/step - loss: 1.0000 - hinge_accuracy: 0.6016 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.0000 - hinge_accuracy: 0.4453 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 1.0000 - hinge_accuracy: 0.5000 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.0000 - hinge_accuracy: 0.4688 - val_loss: 1.0000 - val_hinge_accuracy: 0.3906\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 1.0000 - hinge_accuracy: 0.5078 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.0000 - hinge_accuracy: 0.5000 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 1.0000 - hinge_accuracy: 0.4297 - val_loss: 1.0000 - val_hinge_accuracy: 0.3906\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.0000 - hinge_accuracy: 0.5000 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.0000 - hinge_accuracy: 0.4453 - val_loss: 1.0000 - val_hinge_accuracy: 0.3906\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.0000 - hinge_accuracy: 0.3828 - val_loss: 1.0000 - val_hinge_accuracy: 0.6094\n",
      "{'loss': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'hinge_accuracy': [0.6015625, 0.4453125, 0.5, 0.46875, 0.5078125, 0.5, 0.4296875, 0.5, 0.4453125, 0.3828125], 'val_loss': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_hinge_accuracy': [0.609375, 0.609375, 0.609375, 0.390625, 0.609375, 0.609375, 0.390625, 0.609375, 0.390625, 0.609375]}\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0000 - hinge_accuracy: 0.6094\n",
      "Test Loss: 1.0\n",
      "Test Accuracy: 0.609375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import matplotlib.pyplot as plt\n",
    "import cirq\n",
    "import sympy\n",
    "\n",
    "def hinge_accuracy(y_true, y_pred):\n",
    "    y_true = tf.squeeze(y_true) > 0.0\n",
    "    y_pred = tf.squeeze(y_pred) > 0.0\n",
    "    result = tf.cast(y_true == y_pred, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(result)\n",
    "\n",
    "def convert_to_circuit(values):\n",
    "    \"\"\"Converts the input data into a quantum circuit.\"\"\"\n",
    "    qubits = cirq.GridQubit.rect(1, len(values))\n",
    "    circuit = cirq.Circuit()\n",
    "    for i, value in enumerate(values):\n",
    "        if value:\n",
    "            circuit.append(cirq.X(qubits[i]))\n",
    "    return circuit\n",
    "\n",
    "class CircuitLayerBuilder():\n",
    "    def __init__(self, data_qubits, readout):\n",
    "        self.data_qubits = data_qubits\n",
    "        self.readout = readout\n",
    "\n",
    "    def add_layer(self, circuit, gate, prefix, entangle_readout=True):\n",
    "        for i, qubit in enumerate(self.data_qubits):\n",
    "            symbol = sympy.Symbol(prefix + ':' + str(i))\n",
    "            # Modify Logic here\n",
    "            if entangle_readout:\n",
    "                circuit.append(gate(qubit, self.readout)**symbol)\n",
    "            else:\n",
    "                # circuit.append(gate(symbol)(qubit))\n",
    "                circuit.append(gate(qubit)**symbol)\n",
    "            \n",
    "\n",
    "def create_quantum_circuit(qubits, readout):\n",
    "    circuit = cirq.Circuit()\n",
    "    builder = CircuitLayerBuilder(data_qubits=qubits, readout=readout)\n",
    "    \n",
    "    # Adding layers\n",
    "    # builder.add_layer(circuit, gate=cirq.XX, prefix=f'XX0')\n",
    "    builder.add_layer(circuit, gate=cirq.X, prefix=f'RY1', entangle_readout=False)\n",
    "    builder.add_layer(circuit, gate=cirq.Y, prefix=f'RX1', entangle_readout=False)\n",
    "    builder.add_layer(circuit, gate=cirq.Z, prefix=f'RZ1', entangle_readout=False)\n",
    "    # builder.add_layer(circuit, gate=cirq.ZZ, prefix=f'ZZ2')\n",
    "    # builder.add_layer(circuit, gate=cirq.rz, prefix=f'rz{layer}')\n",
    "    # Adding entanglement\n",
    "    for i in range(len(qubits)-1):\n",
    "        circuit.append(cirq.CNOT(qubits[i], qubits[i+1]))\n",
    "    circuit.append(cirq.CNOT(qubits[-1], readout))\n",
    "    circuit.append(cirq.H(readout))\n",
    "    return circuit, cirq.Z(readout)\n",
    "\n",
    "# Adjust the layer in quantum layer creation\n",
    "def create_quantum_model(qubits, readout):\n",
    "    data_input = tf.keras.Input(shape = (), dtype = tf.dtypes.string)\n",
    "    \n",
    "    model_circuit, model_readout = create_quantum_circuit(qubits, readout)\n",
    "    print(model_circuit)\n",
    "    \n",
    "    quantum_layer = tfq.layers.PQC(model_circuit, model_readout)(data_input)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [data_input], outputs = [quantum_layer])\n",
    "    \n",
    "    return model\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, 5)  # Define 5 qubits\n",
    "readout = cirq.LineQubit(0)\n",
    "qnn_model = create_quantum_model(qubits, readout)\n",
    "\n",
    "# Compile, fit, evaluate model as before\n",
    "qnn_model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss=tf.keras.losses.Hinge(), \n",
    "                  metrics=[hinge_accuracy])\n",
    "\n",
    "qnn_model.summary()\n",
    "\n",
    "\n",
    "X_train_circuits = [convert_to_circuit(x) for x in X_train]\n",
    "X_test_circuits = [convert_to_circuit(x) for x in X_test]\n",
    "\n",
    "# Note: Make sure y_train and y_test are properly defined and preprocessed\n",
    "history = qnn_model.fit(tfq.convert_to_tensor(X_train_circuits), y_train_hinge, \n",
    "              batch_size=32, verbose=1, epochs=10, \n",
    "              validation_data=(tfq.convert_to_tensor(X_test_circuits), y_test_hinge))\n",
    "\n",
    "print(history.history)\n",
    "plt.plot(history.history['val_hinge_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Correct the evaluate call to use the quantum tensor format\n",
    "test_loss, test_accuracy = qnn_model.evaluate(tfq.convert_to_tensor(X_test_circuits), y_test_hinge)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "\n",
    "def add_one_qubit_unitary(circuit, qubits, prefix, params):\n",
    "    if params is None:\n",
    "        for i, qubit in enumerate(qubits):\n",
    "            circuit.append(cirq.X(qubit)**sympy.Symbol(f'{prefix}_{3*i}'))\n",
    "            circuit.append(cirq.Y(qubit)**sympy.Symbol(f'{prefix}_{3*i+1}'))\n",
    "            circuit.append(cirq.Z(qubit)**sympy.Symbol(f'{prefix}_{3*i+2}'))\n",
    "    else:   \n",
    "        for i, qubit in enumerate(qubits):\n",
    "            circuit.append(cirq.X(qubit)**params[3*i])\n",
    "            circuit.append(cirq.Y(qubit)**params[3*i+1])\n",
    "            circuit.append(cirq.Z(qubit)**params[3*i+2])\n",
    "\n",
    "def add_entangling_layer(circuit, qubits):\n",
    "    for i in range(len(qubits) - 1):\n",
    "        circuit.append(cirq.CNOT(qubits[i], qubits[i + 1]))\n",
    "\n",
    "def generator(gen_qubits, data_qubits, layer, init_weights=None):\n",
    "    circuit = cirq.Circuit()\n",
    "    \n",
    "    qubits = gen_qubits + data_qubits\n",
    "    \n",
    "    # Apply RY gates with random angles to the first half of qubits\n",
    "    random_angles = np.random.normal(loc=0, scale=np.pi/3, size=len(qubits))\n",
    "    for i, angle in enumerate(random_angles):\n",
    "        circuit.append(cirq.ry(angle)(qubits[i]))\n",
    "\n",
    "    # Add layers\n",
    "    params = None\n",
    "    for i in range(layer):\n",
    "        if init_weights is not None:\n",
    "            params = init_weights[3*len(qubits)*i:3*len(qubits)*(i+1)]\n",
    "        add_one_qubit_unitary(circuit, qubits, f'gen_{i}', params)\n",
    "        add_entangling_layer(circuit, qubits)\n",
    "    \n",
    "    if init_weights is not None:\n",
    "        params = init_weights[-3*len(output_qubits):]\n",
    "\n",
    "    add_one_qubit_unitary(circuit, data_qubits, f'gen_out', params)\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "def discriminator(data_qubits, output_qubits, layer, init_weights=None):\n",
    "    circuit = cirq.Circuit()\n",
    "    qubits = data_qubits + output_qubits\n",
    "\n",
    "    # Hadamard layer for data qubits\n",
    "    for qubit in data_qubits:\n",
    "        circuit.append(cirq.H(qubit))\n",
    "\n",
    "    # Add layers\n",
    "    params = None\n",
    "    for i in range(layer):\n",
    "        if init_weights is not None:\n",
    "            params = init_weights[3*len(qubits)*i:3*len(qubits)*(i+1)]\n",
    "        add_one_qubit_unitary(circuit, qubits, f'disc_{i}', params)\n",
    "        add_entangling_layer(circuit, qubits)\n",
    "        \n",
    "    if init_weights is not None:\n",
    "        params = init_weights[-3*len(output_qubits):]\n",
    "        \n",
    "    add_one_qubit_unitary(circuit, output_qubits, f'disc_out', params)\n",
    "    # if init_weights is not None:\n",
    "    #     for i in range(layer):\n",
    "    #         values = init_weights[3*len(qubits)*i:3*len(qubits)*(i+1)]\n",
    "    #         add_one_qubit_unitary(circuit, qubits, values)\n",
    "    #         add_entangling_layer(circuit, qubits)\n",
    "            \n",
    "    #     add_one_qubit_unitary(circuit, output_qubits, init_weights[-3*len(output_qubits):])\n",
    "        \n",
    "    # else:\n",
    "    #     for i in range(layer):\n",
    "    #         add_one_qubit_unitary(circuit, qubits, f'disc_{i}')\n",
    "    #         add_entangling_layer(circuit, qubits)\n",
    "            \n",
    "    #     add_one_qubit_unitary(circuit, output_qubits, f'disc_out')\n",
    "\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def create_gen_disc_circuit(qubits, num_features, num_classes, gen_layer, disc_layer, init_weights=None):\n",
    "    gen_disc_circuit = cirq.Circuit()\n",
    "\n",
    "    # Assuming the division of qubits for generator and discriminator is predefined\n",
    "    gen_qubits = qubits[:num_features]\n",
    "    data_qubits = qubits[num_features:-(num_classes+1)]  # Adjust as necessary\n",
    "    output_qubits = qubits[-(num_classes+1):]\n",
    "\n",
    "    # Add the generator and discriminator circuits\n",
    "    gen_disc_circuit += generator(gen_qubits, data_qubits, gen_layer)\n",
    "    gen_disc_circuit += discriminator(data_qubits, output_qubits, disc_layer, init_weights)\n",
    "\n",
    "    return gen_disc_circuit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0): ────Ry(-0.628π)───X^gen_0_0────Y^gen_0_1────Z^gen_0_2────@───X^gen_1_0───Y^gen_1_1───Z^gen_1_2───────────────@──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                 │                                                   │\n",
      "(0, 1): ────Ry(-0.232π)───X^gen_0_3────Y^gen_0_4────Z^gen_0_5────X───@───────────X^gen_1_3───Y^gen_1_4───Z^gen_1_5───X────────────@─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                     │                                                            │\n",
      "(0, 2): ────Ry(-0.064π)───X^gen_0_6────Y^gen_0_7────Z^gen_0_8────────X───────────@───────────X^gen_1_6───Y^gen_1_7───Z^gen_1_8────X────────────@────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                 │                                                             │\n",
      "(0, 3): ────Ry(-0.554π)───X^gen_0_9────Y^gen_0_10───Z^gen_0_11───────────────────X───────────@───────────X^gen_1_9───Y^gen_1_10───Z^gen_1_11───X────────────@───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                             │                                                              │\n",
      "(0, 4): ────Ry(0.592π)────X^gen_0_12───Y^gen_0_13───Z^gen_0_14───────────────────────────────X───────────@───────────X^gen_1_12───Y^gen_1_13───Z^gen_1_14───X────────────@──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                         │                                                               │\n",
      "(0, 5): ────Ry(-0.199π)───X^gen_0_15───Y^gen_0_16───Z^gen_0_17───────────────────────────────────────────X───────────@────────────X^gen_1_15───Y^gen_1_16───Z^gen_1_17───X────────────@────────────X^gen_out_0───Y^gen_out_1───Z^gen_out_2────────────────────────────────────────────────H─────────────X^disc_0_0────Y^disc_0_1────Z^disc_0_2────@───X^disc_1_0───Y^disc_1_1───Z^disc_1_2────────────────@─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                                     │                                                                │                                                                                                                                                           │                                                       │\n",
      "(0, 6): ────Ry(-0.401π)───X^gen_0_18───Y^gen_0_19───Z^gen_0_20───────────────────────────────────────────────────────X────────────@────────────X^gen_1_18───Y^gen_1_19───Z^gen_1_20───X────────────@─────────────X^gen_out_3───Y^gen_out_4───Z^gen_out_5──────────────────────────────────H─────────────X^disc_0_3────Y^disc_0_4────Z^disc_0_5────X───@────────────X^disc_1_3───Y^disc_1_4───Z^disc_1_5───X─────────────@───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                                                  │                                                                │                                                                                                                                                  │                                                                 │\n",
      "(0, 7): ────Ry(-0.419π)───X^gen_0_21───Y^gen_0_22───Z^gen_0_23────────────────────────────────────────────────────────────────────X────────────@────────────X^gen_1_21───Y^gen_1_22───Z^gen_1_23───X─────────────@─────────────X^gen_out_6───Y^gen_out_7────Z^gen_out_8───────────────────H─────────────X^disc_0_6────Y^disc_0_7────Z^disc_0_8────────X────────────@────────────X^disc_1_6───Y^disc_1_7───Z^disc_1_8────X─────────────@─────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                                                               │                                                                 │                                                                                                                                                 │                                                                  │\n",
      "(0, 8): ────Ry(0.236π)────X^gen_0_24───Y^gen_0_25───Z^gen_0_26─────────────────────────────────────────────────────────────────────────────────X────────────@────────────X^gen_1_24───Y^gen_1_25───Z^gen_1_26────X─────────────@─────────────X^gen_out_9────Y^gen_out_10───Z^gen_out_11───H─────────────X^disc_0_9────Y^disc_0_10───Z^disc_0_11────────────────────X────────────@────────────X^disc_1_9───Y^disc_1_10───Z^disc_1_11───X─────────────@───────────────────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                                                                            │                                                                  │                                                                                                                                                │                                                                   │\n",
      "(0, 9): ────Ry(-0.342π)───X^gen_0_27───Y^gen_0_28───Z^gen_0_29──────────────────────────────────────────────────────────────────────────────────────────────X────────────X^gen_1_27───Y^gen_1_28───Z^gen_1_29──────────────────X─────────────X^gen_out_12───Y^gen_out_13───Z^gen_out_14───H─────────────X^disc_0_12───Y^disc_0_13───Z^disc_0_14─────────────────────────────────X────────────@────────────X^disc_1_12───Y^disc_1_13───Z^disc_1_14───X─────────────@─────────────────────────────────────────────────────────────────────────────\n",
      "                                                                                                                                                                                                                                                                                                                                                                                             │                                                                    │\n",
      "(0, 10): ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────X^disc_0_15───Y^disc_0_16───Z^disc_0_17────────────────────────────────────────────────────────────X────────────@─────────────X^disc_1_15───Y^disc_1_16───Z^disc_1_17───X─────────────@───X^disc_out_0───Y^disc_out_1───Z^disc_out_2──────────────────\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                          │                                                                     │\n",
      "(0, 11): ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────X^disc_0_18───Y^disc_0_19───Z^disc_0_20─────────────────────────────────────────────────────────────────────────X─────────────@─────────────X^disc_1_18───Y^disc_1_19───Z^disc_1_20───X───@──────────────X^disc_out_3───Y^disc_out_4───Z^disc_out_5───\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                        │                                                           │\n",
      "(0, 12): ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────X^disc_0_21───Y^disc_0_22───Z^disc_0_23───────────────────────────────────────────────────────────────────────────────────────X─────────────X^disc_1_21───Y^disc_1_22───Z^disc_1_23───────X──────────────X^disc_out_6───Y^disc_out_7───Z^disc_out_8───\n"
     ]
    }
   ],
   "source": [
    "# Create the circuit\n",
    "qubits = cirq.GridQubit.rect(1, 13)\n",
    "num_features = 5  # Assuming 5 qubits for generator/data\n",
    "num_classes = 2  # Assuming 3 qubits for discriminator output\n",
    "gen_layer = 2\n",
    "disc_layer = 2\n",
    "\n",
    "gen_disc_circuit = create_gen_disc_circuit(qubits, num_features, num_classes, gen_layer, disc_layer)\n",
    "print(gen_disc_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "alpha_weight = 0.50  # Weight for the classifier loss\n",
    "\n",
    "@tf.function\n",
    "def disc_loss(y_true, y_pred):\n",
    "    D_true, D_pred = (y_true[:, 2] + 1) / 2, (y_pred[:, 2] + 1) / 2\n",
    "    D_loss = tf.reduce_mean(-1 * (tf.math.log(D_pred + 1e-10) * D_true + tf.math.log(1 - D_pred + 1e-10) * (1 - D_true)))\n",
    "    C_true, C_pred = (y_true[:, :2] + 1) / 2, (y_pred[:, :2] + 1) / 2\n",
    "    D_true_size = tf.cast(tf.size(tf.where(D_true == 1)), dtype=tf.float32) + 1e-10\n",
    "    C_loss = tf.reduce_sum(tf.keras.losses.CategoricalCrossentropy(reduction='none')(C_true, C_pred) * D_true) / D_true_size\n",
    "    return (1 - alpha_weight) * D_loss + alpha_weight * C_loss\n",
    "\n",
    "@tf.function\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    D_true = (y_true[:, 2] + 1) / 2\n",
    "    C_real_true, C_real_pred = tf.math.argmax((y_true[:, :2] + 1) / 2, axis=1), tf.math.argmax((y_pred[:, :2] + 1) / 2, axis=1)\n",
    "    same = tf.cast(C_real_true == C_real_pred, tf.float32) * D_true\n",
    "    D_true_size = tf.cast(tf.size(tf.where(D_true == 1)), dtype=tf.float32) + 1e-10\n",
    "    return tf.reduce_sum(same) / D_true_size\n",
    "\n",
    "\n",
    "def gen_loss(y_true, y_pred):\n",
    "    y_pred = (y_pred + 1) / 2\n",
    "    return tf.reduce_mean((-1) * tf.math.log(y_pred + 1e-10), axis=0)\n",
    "\n",
    "def generate_identity(X):\n",
    "    return [cirq.Circuit() for _ in X]\n",
    "\n",
    "def generate_data(X, qubits):\n",
    "    quantum_data = []\n",
    "    for sample in X:\n",
    "        circuit = cirq.Circuit([cirq.ry(sample[bit])(qubits[bit]) for bit in range(len(sample))])\n",
    "        quantum_data.append(circuit)\n",
    "    return quantum_data\n",
    "\n",
    "def checkpoints(cycle):\n",
    "    gen_model_cp = tf.keras.callbacks.ModelCheckpoint(filepath='./model_save/cp_generator1_' + str(cycle) + '.h5', save_weights_only=True, monitor='loss', mode='min', save_best_only=True)\n",
    "    disc_model_cp = tf.keras.callbacks.ModelCheckpoint(filepath='./model_save/cp_disc1_' + str(cycle) + '.h5', save_weights_only=True, monitor='custom_accuracy', mode='max', save_best_only=True)\n",
    "    return gen_model_cp, disc_model_cp\n",
    "\n",
    "def train_qgen(epochs, batch, verbose):\n",
    "    return qgen_model.fit(x=identity_data, y=identity_label, batch_size=batch, epochs=epochs, verbose=verbose)\n",
    "\n",
    "def train_qdisc(epochs, batch, verbose, disc_model_cp, gen_data_train, y_gen_train, gen_data_test, y_gen_test):\n",
    "    return qdisc_model.fit(x=gen_data_train, y=y_gen_train, batch_size=batch, epochs=epochs, verbose=verbose, callbacks=[disc_model_cp], validation_data=(gen_data_test, y_gen_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(qubits, disc_layer):\n",
    "    disc_readout_operators = [cirq.Z(qubits[-(num_classes+1) + q]) for q in range(num_classes+1)]\n",
    "    \n",
    "    data_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)\n",
    "\n",
    "    pqc = tfq.layers.PQC(discriminator(qubits[num_features:-(num_classes+1)], qubits[-(num_classes+1):], disc_layer), disc_readout_operators)(data_input)\n",
    "    \n",
    "    # Define and compile the model\n",
    "    model = tf.keras.Model(inputs=[data_input], outputs=[pqc])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=disc_loss,\n",
    "                  metrics=[custom_accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generator_model(qubits, gen_layer, init_weights):\n",
    "    gen_readout_operators = [cirq.Z(qubits[-1])] \n",
    "    \n",
    "    data_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)\n",
    "\n",
    "    pqc = tfq.layers.PQC(create_gen_disc_circuit(qubits, num_features, num_classes, gen_layer, disc_layer, init_weights=init_weights), gen_readout_operators)(data_input)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[data_input], outputs=[pqc])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=gen_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " pqc_13 (PQC)                (None, 3)                 57        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qdisc_model = discriminator_model(qubits, disc_layer)\n",
    "qdisc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " pqc_14 (PQC)                (None, 1)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75\n",
      "Trainable params: 75\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qgen_model = generator_model(qubits, gen_layer, qdisc_model.get_weights()[0])\n",
    "qgen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 (100,)\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3802\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 0.3799\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 893ms/step - loss: 0.3796\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 976ms/step - loss: 0.3792\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 996ms/step - loss: 0.3789\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.3786\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 0.3782\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.3779\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.3776\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 0.3773\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 885ms/step - loss: 0.3769\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.3766\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 821ms/step - loss: 0.3763\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 0.3760\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.3756\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.3753\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.3750\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 0.3746\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.3743\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 0.3740\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 0.3736\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 0.3733\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.3729\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.3726\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.3722\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.3718\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.3715\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.3711\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.3708\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.3704\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.3700\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.3697\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 0.3693\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.3690\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.3686\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.3682\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.3679\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.3675\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 0.3672\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.3669\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.3665\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.3662\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.3659\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.3655\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.3652\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.3649\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 0.3646\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.3643\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.3640\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.3637\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 1s 594ms/step - loss: 0.3634\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 0.3631\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 0.3628\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.3626\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.3623\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.3620\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.3617\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 0.3615\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 0.3612\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.3609\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.3607\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.3604\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.3602\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.3599\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 0.3596\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.3594\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.3591\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 0.3589\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.3587\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.3584\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.3582\n"
     ]
    }
   ],
   "source": [
    "## First training\n",
    "# \n",
    "# \n",
    "# Convert generated identity quantum data to TensorFlow Quantum tensors\n",
    "identity_data = tfq.convert_to_tensor(generate_identity(X_train))\n",
    "\n",
    "# Create identity labels (all zeros) corresponding to each identity data sample\n",
    "# this label will be used in the quantum fake data \n",
    "identity_label = np.zeros((len(identity_data),))\n",
    "\n",
    "# Print the lengths of identity data and identity labels for sanity check\n",
    "print(len(identity_data), identity_label.shape)\n",
    "# %%\n",
    "# Set the weight for the discriminator loss in the combined loss function\n",
    "alpha_weight = 0.50\n",
    "\n",
    "# Retrieve the best weights of the discriminator and generator models\n",
    "best_qdisc_weights = qdisc_model.get_weights()[0]\n",
    "best_qgen_weights = qgen_model.get_weights()[0]\n",
    "\n",
    "# Re-declare the generator model using the weights of the discriminator model\n",
    "qgen_model = generator_model(qubits, gen_layer, qdisc_model.get_weights()[0])\n",
    "\n",
    "\n",
    "# %%\n",
    "# Create model checkpoint callbacks for generator and discriminator models\n",
    "gen_model_cp, disc_model_cp = checkpoints(cycle=1)\n",
    "# gen_model_cp.summary()\n",
    "\n",
    "# %%\n",
    "# Fit the Generator Model\n",
    "H = train_qgen(epochs=1000, batch=100, verbose=1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "# Plot the generator's loss\n",
    "plt.plot(H.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"1. Generator's Loss.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 200\n",
      "Number of samples in testing set: 200\n",
      "Shapes of labels for training and testing sets: (200, 3) (200, 3)\n"
     ]
    }
   ],
   "source": [
    "def generate_fake_data(X, qubits, symbols, layer=1):\n",
    "    \"\"\"Generate fake quantum data.\"\"\"\n",
    "    quantum_data = []  # List to store fake quantum data circuits\n",
    "    \n",
    "    # Iterate through data samples (X is only used to determine the number of fake samples to generate)\n",
    "    for sample in X:\n",
    "        # Generate a fake quantum data circuit using the generator model\n",
    "        # The generator is applied to the specified qubits and with the given symbols and layer\n",
    "        circuit = generator(qubits[:len(sample)], qubits[len(sample):2*len(sample)], qgen_model.get_weights()[0], layer=layer)\n",
    "        \n",
    "        quantum_data.append(circuit)  # Append the generated quantum data circuit to the list\n",
    "    \n",
    "    return quantum_data  # Return the list of fake quantum data circuits\n",
    "\n",
    "# Convert generated fake quantum data to TensorFlow Quantum tensors\n",
    "fake_data = tfq.convert_to_tensor(generate_fake_data(X_train, qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "\n",
    "# Create labels for the fake data\n",
    "# Initialize an array of zeros for the labels with shape (number of fake data samples, number of classes + 1)\n",
    "y_true_fake = np.zeros((len(fake_data), num_classes + 1))\n",
    "\n",
    "# Set the third column of the labels to -1, indicating that the data is fake\n",
    "y_true_fake[:, 2] += (-1)\n",
    "\n",
    "# Generate Real + Fake Data for training and testing sets\n",
    "gen_data_train = tfq.convert_to_tensor(generate_data(X_train, qubits) + generate_fake_data(X_train, qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "gen_data_test = tfq.convert_to_tensor(generate_data(X_test, qubits) + generate_fake_data(X_test, qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate labels for training and testing sets\n",
    "y_gen_train = np.concatenate((y_train_hinge, y_true_fake), axis=0)\n",
    "y_gen_test = np.concatenate((y_test_hinge, y_true_fake), axis=0)\n",
    "\n",
    "# Display information about generated data\n",
    "print(\"Number of samples in training set:\", len(gen_data_train))\n",
    "print(\"Number of samples in testing set:\", len(gen_data_test))\n",
    "print(\"Shapes of labels for training and testing sets:\", y_gen_train.shape, y_gen_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generate Real + Fake Data for training and testing sets\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m gen_data_train \u001b[38;5;241m=\u001b[39m tfq\u001b[38;5;241m.\u001b[39mconvert_to_tensor(generate_data(\u001b[43mx_train\u001b[49m, qgan_qubits) \u001b[38;5;241m+\u001b[39m generate_fake_data(x_train, qgan_qubits, qgen_model\u001b[38;5;241m.\u001b[39mget_weights()[\u001b[38;5;241m0\u001b[39m], layer\u001b[38;5;241m=\u001b[39mgen_layer))\n\u001b[1;32m      4\u001b[0m gen_data_test \u001b[38;5;241m=\u001b[39m tfq\u001b[38;5;241m.\u001b[39mconvert_to_tensor(generate_data(x_test, qgan_qubits) \u001b[38;5;241m+\u001b[39m generate_fake_data(x_test, qgan_qubits, qgen_model\u001b[38;5;241m.\u001b[39mget_weights()[\u001b[38;5;241m0\u001b[39m], layer\u001b[38;5;241m=\u001b[39mgen_layer))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Concatenate labels for training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Generate Real + Fake Data for training and testing sets\n",
    "gen_data_train = tfq.convert_to_tensor(generate_data(x_train, qgan_qubits) + generate_fake_data(x_train, qgan_qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "gen_data_test = tfq.convert_to_tensor(generate_data(x_test, qgan_qubits) + generate_fake_data(x_test, qgan_qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "\n",
    "# Concatenate labels for training and testing sets\n",
    "y_gen_train = np.concatenate((y_train, y_true_fake), axis=0)\n",
    "y_gen_test = np.concatenate((y_test, y_true_fake), axis=0)\n",
    "\n",
    "# Display information about generated data\n",
    "print(\"Number of samples in training set:\", len(gen_data_train))\n",
    "print(\"Number of samples in testing set:\", len(gen_data_test))\n",
    "print(\"Shapes of labels for training and testing sets:\", y_gen_train.shape, y_gen_test.shape)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(tfq.from_tensor(gen_data_train)[0])\n",
    "\n",
    "# %%\n",
    "# Fit the Discriminator Model\n",
    "H = train_qdisc(epochs = 200, batch = 64, verbose = 1)\n",
    "\n",
    "# %%\n",
    "# Plot the discriminator's loss over epochs\n",
    "plt.plot(H.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"1. Discriminator's Loss. alpha_c = 0.5\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot the discriminator's accuracy over epochs\n",
    "plt.plot(H.history['custom_accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"1. Discriminator's Accuracy. alpha_c = 0.5\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# This plot illustrates the discriminator's accuracy in distinguishing between real and fake data, as well as differentiating signal from background events. The model's performance is evaluated across both classification tasks, with equal emphasis on each.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Second Training\n",
    "\n",
    "# %% [markdown]\n",
    "# By increasing $\\alpha_C$ to $0.75$, we are assigning greater importance to signal/background events over real/fake data classification.\n",
    "\n",
    "# %%\n",
    "# Update the alpha weight parameter to 0.75\n",
    "alpha_weight = 0.75\n",
    "\n",
    "# Retrieve the weights of the discriminator and generator models\n",
    "best_qdisc_weights = qdisc_model.get_weights()[0]\n",
    "best_qgen_weights = qgen_model.get_weights()[0]\n",
    "\n",
    "# Re-declare the generator model using the discriminator's weights\n",
    "qgen_model = generator_model(symbols_gen, qdisc_model.get_weights()[0])\n",
    "\n",
    "\n",
    "# %%\n",
    "gen_model_cp, disc_model_cp = checkpoints(cycle=2)\n",
    "\n",
    "# %%\n",
    "# Fit the Generator Model\n",
    "H = train_qgen(1000, 100, 1)\n",
    "\n",
    "# %%\n",
    "# Plot the generator's loss\n",
    "plt.plot(H.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"#2. Generator's Loss. alpha_c = 0.75\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Generate real and fake quantum data for training and testing\n",
    "\n",
    "# Generate real and fake quantum data for training set\n",
    "gen_data_train = tfq.convert_to_tensor(generate_data(x_train, qgan_qubits) + generate_fake_data(x_train, qgan_qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "\n",
    "# Generate real and fake quantum data for testing set\n",
    "gen_data_test = tfq.convert_to_tensor(generate_data(x_test, qgan_qubits) + generate_fake_data(x_test, qgan_qubits, qgen_model.get_weights()[0], layer=gen_layer))\n",
    "\n",
    "y_train = np.concatenate((y_train, np.ones((len(y_train), 1))), axis=1)\n",
    "y_test = np.concatenate((y_test, np.ones((len(y_test), 1))), axis=1)\n",
    "\n",
    "# Concatenate labels for training and testing sets\n",
    "y_gen_train = np.concatenate((y_train, y_true_fake), axis=0)\n",
    "y_gen_test = np.concatenate((y_test, y_true_fake), axis=0)\n",
    "\n",
    "# Print the lengths of the training and testing data\n",
    "print(\"Number of samples in training set:\", len(gen_data_train))\n",
    "print(\"Number of samples in testing set:\", len(gen_data_test))\n",
    "\n",
    "# Print the shapes of the labels for training and testing sets\n",
    "print(\"Shape of labels for training set:\", y_gen_train.shape)\n",
    "print(\"Shape of labels for testing set:\", y_gen_test.shape)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Fit the Discriminator Model\n",
    "H = train_qdisc(200, 64, 1)\n",
    "\n",
    "# %%\n",
    "# Plot the discriminator's loss over epochs\n",
    "plt.plot(H.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"#2. Discriminator's Loss. alpha_c = 0.75\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot the discriminator's accuracy over epochs\n",
    "plt.plot(H.history['custom_accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"#2. Discriminator's Accuracy. alpha_c = 0.75\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Training Accuracy:\", custom_accuracy(np.array(y_train, dtype=np.float32), qdisc_model.predict(train_quantum_data)).numpy())\n",
    "print(\"Testing Accuracy\", custom_accuracy(np.array(y_test, dtype=np.float32), qdisc_model.predict(test_quantum_data)).numpy())\n",
    "\n",
    "print(\"Training AUC:\", roc_auc_score(np.argmax(((y_train+1)/2)[:, :2], axis=1), (((qdisc_model.predict(train_quantum_data)+1)/2)[:, :2])[:, 1]))\n",
    "print(\"Testing AUC:\", roc_auc_score(np.argmax(((y_test+1)/2)[:, :2], axis=1), (((qdisc_model.predict(test_quantum_data)+1)/2)[:, :2])[:, 1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
